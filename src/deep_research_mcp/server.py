"""MCP server exposing the OpenAI Deep Research API."""

import logging
import os
from typing import List, Dict, Iterable, Optional

from openai import AsyncOpenAI
from fastmcp import FastMCP
from fastmcp.contrib.bulk_tool_caller import BulkToolCaller

logger = logging.getLogger(__name__)

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
VECTOR_STORE_ID = os.environ.get("VECTOR_STORE_ID", "")

DEFAULT_MODEL = os.environ.get("DEEP_RESEARCH_MODEL", "o4-mini-deep-research-2025-06-26")
DEFAULT_SYSTEM_PROMPT = os.environ.get(
    "DEEP_RESEARCH_SYSTEM_PROMPT",
    "You are an expert researcher providing thorough, citation-backed answers."
)
DEFAULT_TOOLS = os.environ.get("DEEP_RESEARCH_TOOLS", "web_search_preview")


def _parse_tools(tools: Optional[Iterable[str]] | str | None) -> List[Dict]:
    if tools is None:
        tool_names = [t.strip() for t in DEFAULT_TOOLS.split(',') if t.strip()]
    elif isinstance(tools, str):
        tool_names = [t.strip() for t in tools.split(',') if t.strip()]
    else:
        tool_names = [t.strip() for t in tools if t]

    result: List[Dict] = []
    for name in tool_names:
        if name == 'code_interpreter':
            result.append({"type": "code_interpreter", "container": {"type": "auto", "file_ids": []}})
        else:
            result.append({"type": name})
    return result


_client = AsyncOpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None


mcp = FastMCP("deep-research-mcp")

_bulk_tools = BulkToolCaller()
_bulk_tools.register_tools(mcp)


async def _search_vector_store(query: str):
    if not _client:
        raise RuntimeError("OPENAI_API_KEY not configured")
    resp = _client.vector_stores.search(vector_store_id=VECTOR_STORE_ID, query=query)
    results = []
    async for item in resp:
        results.append(item)
    return results


async def _fetch_file(file_id: str) -> str:
    if not _client:
        raise RuntimeError("OPENAI_API_KEY not configured")
    pager = _client.vector_stores.files.content(
        vector_store_id=VECTOR_STORE_ID, file_id=file_id
    )
    parts = []
    async for chunk in pager:
        parts.append(getattr(chunk, "text", ""))
    return "\n".join(parts)


async def _run_deep_research(
    question: str,
    *,
    model: Optional[str] = None,
    system_prompt: Optional[str] = None,
    tools: Optional[Iterable[str] | str | List[Dict]] = None,
) -> str:
    """Call the Deep Research API and return the report text."""
    if not _client:
        raise RuntimeError("OPENAI_API_KEY not configured")

    selected_model = model or DEFAULT_MODEL
    prompt = system_prompt or DEFAULT_SYSTEM_PROMPT
    tool_payload = tools if isinstance(tools, list) and tools and isinstance(tools[0], dict) else _parse_tools(tools)

    resp = await _client.responses.create(
        model=selected_model,
        input=[
            {
                "role": "developer",
                "content": [{"type": "input_text", "text": prompt}],
            },
            {
                "role": "user",
                "content": [{"type": "input_text", "text": question}],
            },
        ],
        reasoning={"summary": "auto"},
        tools=tool_payload,
        background=True,
    )
    return resp.output[-1].content[0].text


@mcp.tool()
async def search_papers(query: str) -> str:
    """Search vector store for papers matching ``query``."""
    items = await _search_vector_store(query)
    if not items:
        return "No papers found"
    lines = [
        f"{getattr(i, 'filename', 'Untitled')} ({getattr(i, 'file_id', '')})"
        for i in items[:10]
    ]
    if len(items) > 10:
        lines.append(f"...and {len(items) - 10} more results")
    return "\n".join(lines)


@mcp.tool()
async def get_paper_summary(paper_id: str) -> str:
    """Fetch the full text for ``paper_id`` and return a short snippet."""
    text = await _fetch_file(paper_id)
    return text[:500] if text else "No summary available"


@mcp.tool()
async def research_summary(
    question: str,
    system_prompt: Optional[str] | None = None,
    model: Optional[str] | None = None,
    tools: Optional[Iterable[str] | str | List[Dict]] = None,
) -> str:
    """Return a short report generated by the Deep Research API."""
    return await _run_deep_research(
        question,
        model=model,
        system_prompt=system_prompt,
        tools=tools,
    )


@mcp.resource("deepresearch://categories")
async def categories() -> str:
    """List research categories if available."""
    return "general"


@mcp.prompt()
def suggest_reading(topic: str) -> str:
    """Prompt text to suggest reading material."""
    return (
        f"Provide recommended research papers about {topic}. "
        "Mention key authors and venues."
    )


def main() -> None:
    """Run the MCP server."""
    logging.basicConfig(level=logging.INFO)
    mcp.run(transport="stdio")


if __name__ == "__main__":
    main()
