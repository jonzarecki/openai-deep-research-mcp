"""MCP server exposing the OpenAI Deep Research API."""

import json
import logging
import os
import re
from pathlib import Path
from typing import Iterable, List, Dict, Optional

from openai import AsyncOpenAI
from fastmcp import FastMCP

logger = logging.getLogger(__name__)

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")

DEFAULT_MODEL = os.environ.get("DEEP_RESEARCH_MODEL", "o4-mini-deep-research-2025-06-26")
DEFAULT_SYSTEM_PROMPT = os.environ.get(
    "DEEP_RESEARCH_SYSTEM_PROMPT",
    "You are an expert researcher providing thorough, citation-backed answers."
)
DEFAULT_TOOLS = os.environ.get("DEEP_RESEARCH_TOOLS", "web_search_preview")

CACHE_PATH = Path(os.environ.get("DEEP_RESEARCH_CACHE_PATH", Path.home() / ".deep_research_cache.json"))
OUTPUT_DIR = Path(os.environ.get("DEEP_RESEARCH_OUTPUT_DIR", "research_logs"))
LOG_FILE = Path(os.environ.get("DEEP_RESEARCH_LOG_FILE", "research.log"))

OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
if LOG_FILE:
    logging.basicConfig(level=logging.INFO, handlers=[logging.FileHandler(LOG_FILE), logging.StreamHandler()])

try:
    with CACHE_PATH.open("r") as f:
        _CACHE: Dict[str, Dict[str, str]] = json.load(f)
except FileNotFoundError:
    _CACHE = {}
except json.JSONDecodeError:
    logger.warning("Corrupted cache file; starting fresh")
    _CACHE = {}


def _parse_tools(tools: Optional[Iterable[str]] | str | None) -> List[Dict]:
    if tools is None:
        tool_names = [t.strip() for t in DEFAULT_TOOLS.split(',') if t.strip()]
    elif isinstance(tools, str):
        tool_names = [t.strip() for t in tools.split(',') if t.strip()]
    else:
        tool_names = [t.strip() for t in tools if t]

    result: List[Dict] = []
    for name in tool_names:
        if name == 'code_interpreter':
            result.append({"type": "code_interpreter", "container": {"type": "auto", "file_ids": []}})
        else:
            result.append({"type": name})
    return result


_client = AsyncOpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None


mcp = FastMCP("deep-research-mcp")


def _slug(text: str) -> str:
    text = re.sub(r"[^a-zA-Z0-9_-]+", "_", text.strip().lower())
    return text[:50] or "result"


def _cache_key(question: str, model: str, prompt: str, tools: List[Dict]) -> str:
    return json.dumps({"q": question, "model": model, "prompt": prompt, "tools": tools}, sort_keys=True)


def _save_cache() -> None:
    with CACHE_PATH.open("w") as f:
        json.dump(_CACHE, f)


async def _run_deep_research(
    question: str,
    *,
    model: Optional[str] = None,
    system_prompt: Optional[str] = None,
    tools: Optional[Iterable[str] | str | List[Dict]] = None,
) -> str:
    """Call the Deep Research API and return the report text."""
    if not _client:
        raise RuntimeError("OPENAI_API_KEY not configured")

    selected_model = model or DEFAULT_MODEL
    prompt = system_prompt or DEFAULT_SYSTEM_PROMPT
    tool_payload = tools if isinstance(tools, list) and tools and isinstance(tools[0], dict) else _parse_tools(tools)

    resp = await _client.responses.create(
        model=selected_model,
        input=[
            {
                "role": "developer",
                "content": [{"type": "input_text", "text": prompt}],
            },
            {
                "role": "user",
                "content": [{"type": "input_text", "text": question}],
            },
        ],
        reasoning={"summary": "auto"},
        tools=tool_payload,
        background=True,
    )
    return resp.output[-1].content[0].text


@mcp.tool()
async def research_summary(
    question: str,
    system_prompt: Optional[str] | None = None,
    model: Optional[str] | None = None,
    tools: Optional[Iterable[str] | str | List[Dict]] = None,
) -> str:
    """Return a short report generated by the Deep Research API.

    Results are cached and written to ``OUTPUT_DIR`` for future reuse.
    """
    selected_model = model or DEFAULT_MODEL
    prompt = system_prompt or DEFAULT_SYSTEM_PROMPT
    tool_payload = tools if isinstance(tools, list) and tools and isinstance(tools[0], dict) else _parse_tools(tools)
    key = _cache_key(question, selected_model, prompt, tool_payload)

    if key in _CACHE:
        logger.info("Returning cached result for query '%s'", question)
        return _CACHE[key]["result"]

    result = await _run_deep_research(
        question,
        model=selected_model,
        system_prompt=prompt,
        tools=tool_payload,
    )
    file_path = OUTPUT_DIR / f"{_slug(question)}.txt"
    file_path.write_text(result)
    logger.info("Saved research result to %s", file_path)
    _CACHE[key] = {"result": result, "path": str(file_path)}
    _save_cache()
    return result


@mcp.tool()
async def get_cached_research(
    question: str,
    system_prompt: Optional[str] | None = None,
    model: Optional[str] | None = None,
    tools: Optional[Iterable[str] | str | List[Dict]] = None,
) -> str:
    """Return a cached research summary without calling the API."""
    selected_model = model or DEFAULT_MODEL
    prompt = system_prompt or DEFAULT_SYSTEM_PROMPT
    tool_payload = tools if isinstance(tools, list) and tools and isinstance(tools[0], dict) else _parse_tools(tools)
    key = _cache_key(question, selected_model, prompt, tool_payload)
    data = _CACHE.get(key)
    if not data:
        raise KeyError("Query not found in cache")
    logger.info("Retrieved cached result for query '%s'", question)
    return data["result"]


def main() -> None:
    """Run the MCP server."""
    mcp.run(transport="stdio")


if __name__ == "__main__":
    main()
