"""MCP server exposing the OpenAI Deep Research API."""

import logging
import os
from typing import Iterable, List, Dict, Optional

from openai import AsyncOpenAI
from fastmcp import FastMCP
from fastmcp.contrib.bulk_tool_caller import BulkToolCaller

logger = logging.getLogger(__name__)

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")

DEFAULT_MODEL = os.environ.get("DEEP_RESEARCH_MODEL", "o4-mini-deep-research-2025-06-26")
DEFAULT_SYSTEM_PROMPT = os.environ.get(
    "DEEP_RESEARCH_SYSTEM_PROMPT",
    "You are an expert researcher providing thorough, citation-backed answers."
)
DEFAULT_TOOLS = os.environ.get("DEEP_RESEARCH_TOOLS", "web_search_preview")


def _parse_tools(tools: Optional[Iterable[str]] | str | None) -> List[Dict]:
    if tools is None:
        tool_names = [t.strip() for t in DEFAULT_TOOLS.split(',') if t.strip()]
    elif isinstance(tools, str):
        tool_names = [t.strip() for t in tools.split(',') if t.strip()]
    else:
        tool_names = [t.strip() for t in tools if t]

    result: List[Dict] = []
    for name in tool_names:
        if name == 'code_interpreter':
            result.append({"type": "code_interpreter", "container": {"type": "auto", "file_ids": []}})
        else:
            result.append({"type": name})
    return result


_client = AsyncOpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None


mcp = FastMCP("deep-research-mcp")

_bulk_tools = BulkToolCaller()
_bulk_tools.register_tools(mcp)


async def _run_deep_research(
    question: str,
    *,
    model: Optional[str] = None,
    system_prompt: Optional[str] = None,
    tools: Optional[Iterable[str] | str | List[Dict]] = None,
) -> str:
    """Call the Deep Research API and return the report text."""
    if not _client:
        raise RuntimeError("OPENAI_API_KEY not configured")

    selected_model = model or DEFAULT_MODEL
    prompt = system_prompt or DEFAULT_SYSTEM_PROMPT
    tool_payload = tools if isinstance(tools, list) and tools and isinstance(tools[0], dict) else _parse_tools(tools)

    resp = await _client.responses.create(
        model=selected_model,
        input=[
            {
                "role": "developer",
                "content": [{"type": "input_text", "text": prompt}],
            },
            {
                "role": "user",
                "content": [{"type": "input_text", "text": question}],
            },
        ],
        reasoning={"summary": "auto"},
        tools=tool_payload,
        background=True,
    )
    return resp.output[-1].content[0].text


@mcp.tool()
async def research_summary(
    question: str,
    system_prompt: Optional[str] | None = None,
    model: Optional[str] | None = None,
    tools: Optional[Iterable[str] | str | List[Dict]] = None,
) -> str:
    """Return a short report generated by the Deep Research API."""
    return await _run_deep_research(
        question,
        model=model,
        system_prompt=system_prompt,
        tools=tools,
    )


def main() -> None:
    """Run the MCP server."""
    logging.basicConfig(level=logging.INFO)
    mcp.run(transport="stdio")


if __name__ == "__main__":
    main()
